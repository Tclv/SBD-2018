# Lab 3 {.unnumbered}

In the third and final lab of SBD we will be implementing a streaming
application. As many of you have noted in the first lab questions, Spark is not
well suited for real-time streaming, because of its batch-processing nature.
Therefore, we will be using *Apache Kafka* for this lab. You will be provided
with a Kafka stream of GDELT records, for which we want you to create a
histogram of the most popular topics of the last hour that will continuously
update. We included another visualizer for this lab that you can see in
[@fig:stream_visualizer].

Apache Kafka is a distributed streaming platform. The core abstraction is that
of a message queue, to which you can both publish and subscribe to streams of
records. Each queue is named by means of a topic. Apache Kafka is:

-   Resilient by means of replication;
-   Scalable on a cluster;
-   High-throughput and low-latency; and
-   A persistent store.

Kafka consists of 4 APIs, from the Kafka docs:

The Producer API
:   allows an application to publish a stream of records to one or more Kafka
    topics.

The Consumer API
:   allows an application to subscribe to one or more topics and process the
    stream of records produced to them.

The Streams API
:   allows an application to act as a stream processor, consuming an input
    stream from one or more topics and producing an output stream to one or
    more output topics, effectively transforming the input streams to output
    streams.

The Connector API
:   allows building and running reusable producers or consumers that connect
    Kafka topics to existing applications or data systems. For example, a
    connector to a relational database might capture every change to a table.

Before you start with the lab please read the [Introduction to Kafka on the Kafka
website](https://kafka.apache.org/intro), to become familiar with the Apache
Kafka abstraction and internals. You can find instructions on how to [install
Kafka on your machine here](https://kafka.apache.org/quickstart).

![Visualizer for the streaming
application](./images/stream_visualizer){#fig:stream_visualizer}

In the lab's repository you will find a template for your solution. There are a
bunch of scripts (`.sh` for MacOS/Linux, `.bat` for Windows). For these scripts
to work you first will have to define a `KAFKA_HOME` environmental variable to
the root of the Kafka installation directory. The Kafka installation directory
should contain the following directories:

```
$ tree .
├── bin
│   └── windows
├── config
├── libs
└── site-docs
```

Once that has been set up, copy the lab files from the GitHub repository. Try
to run the `kafka_start.sh` or `kafka_start.bat` depending on your OS. If you
receive an error about being unable to find a `java` binary, make sure you have
Java installed and it is in your path.

The `kafka_start` script does a number of things:

1. Start a Zookeeper server, which acts as a naming, configuration and task
   coordination server, on port 2181
2. Start a single Kafka broker on port 9092
3. Start the GDelft Kafka Producer, producing the `GDELT` topic
4. Will start a websocket server for the visualizer

We can now inspect the output of the `gdelt` topic by running the following
command on MacOS/Linux
```
$KAFKA_HOME/bin/kafka-console-consumer --bootstrap-server localhost:9092 \
    --topic gdelt_raw --property print.key=true --property key.separator=-
```

or on Windows

```
windows equivalent
```

If you see output appearing, you are now ready to start on the assignment. 

You are now tasked with writing an implementation of the histgoram server.
In the file `GDELTStream/GDELTStream.scala` you will find a number of
unfinished functions that you will have to fill in:

`stream flatmap`
: tbd

`transform`
: tbd

To open the visualizer


## Delivarables

-   A complete zip of the entire project, including your implementation of
    `GDELTStream.scala`
- Report containing
    - Outline of the code (less than 1/2 a page)
    - Answers to the questions listed below

## Questions


